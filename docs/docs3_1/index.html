<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>  
    body {
        padding: 100px;
        width: 1000px;
        margin: auto;
        text-align: left;
        font-weight: 300;
        font-family: 'Open Sans', sans-serif;
        color: #121212;
    }
    h1, h2, h3, h4 {
        font-family: 'Source Sans Pro', sans-serif;
    }
  </style> 
<title>CS 184 PathTracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="style.css" media="screen" />
</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2018</h1>
<h1 align="middle">Assignment 3: PathTracer</h1>
<h2 align="middle">MICHAEL GIBBES, CS184-ABY</h2>

<br><br>

<h2 align="middle">Overview</h2>
<p>The purpose of this project is to add some realistic lighting to the meshes that were enabled by the previous project. In order to implement this lighting, there needs to be some kind of intersection algorithm (part 1). Checking intersection over all the primitives in a scene is egregiously bad, so bounding boxes are also enabled just like in the rasterizer (part 2). The last parts deal with actually tracing the path of the light across scene, simulating some bouncing and also optimizing this at the very end (part 5). It's important to note that all surfaces in this project are currently diffuse, which means that light bounces off of them in random directions to produce a dull, opaque texture.</p>

<h2 align="middle">Part 1: Ray Generation and Intersection</h2>
<p>So begins our raytracing journey!</p>
<p>The raytracing loop generates the specified number of rays, shooting one ray out from the center of the pixel if there is only 1 sample and choosing at random otherwise. In generating each ray, the origin of the ray is placed on a sensor plane at z-value -1 (with appropriate conversions) and the t values are set. Later on, the t value will serve as a marker for primitive precedence in the scene. A bug happened during the implementation of this part where the t value actually wasn't being used correctly, causing a wall that should have been in behind a sphere to be in front of it.</p>
<p>The ray intersection algorithm uses the Moller Trumbore algorithm. This algorithm generates 3 numbers: (1) the intersection coordinate <b>t</b>, (2) the first barycentric coordinate <b>b1</b>, and (3) the second coordinate <b>b2</b>. <b>b3</b> is found by:</p>
<p align="middle"><pre align="middle">b3 = 1 - b1 - b2</pre></p>
<p>Finally, we can test whether the intersection is in the triangle by using the barycentric coordinates (all between 0 and 1) and also test whether another primitive is obstructing the view with the t value. The barycentric coordinates are also useful for shading (i.e. calculating the normal).</p>
<p>Here are a few images generated by the raytracing algorithm:</p>
<div align="middle">
    <table style="width=100%">
        <tr>
            <td>
                <img src="images/part1_image1.png" width="480px" />
                <figcaption align="middle">Sphere Normal Render</figcaption>
            </td>
            <td>
                <img src="images/part1_image2.png" width="480px" />
                <figcaption align="middle">Gems Normal Render</figcaption>
            </td>
        </tr>   
    </table>
</div>
<br>
<div align="middle">
    <table style="width=100%">
        <tr>
            <td>
                <img src="images/part1_image3.png" width="480px" />
                <figcaption align="middle">Cow Normal Render</figcaption>
            </td>
        </tr>
    </table>
</div>
<p>Note that the cow image took 800+ seconds to render with 8 threads on a VM.</p>

<h2 align="middle">Part 2: Bounding Volume Hierarchy</h2>
<p>In this portion we work on accelerating the previous part.</p>
<p>The purpose of the BVH is to focus the intersection checks on small batches to minimize needless checks in blank space. To construct the tree of mesh batches, the algorithm follows the following logic:</p>
<ol>
    <li>Intersect just the bounding box, and if that doesn't work, stop.</li>
    <li>If the current batch is too big, split primitives into two children nodes along the longest axis. If the split is uneven (i.e. one side has no primitives), the bounding box gets resized by half along the longest axis, and then a new axis split is calculated. The new axis might not be the same as the original axis.</li>
    <li>If the current batch is under the max requirement, then intersect every node to find the closest hit.</li>
</ol>
<p>The actual logic for step 1, checking for bounding box intersection, is quite simple. The algorithm checks every axis against the ray's min_t and max_t, updating a local copy of that interval as necessary. The test passes if no check for x, y, or z fails, and the logic is purposefully duplicated so that fails are detected earlier. For example, if we know the x-interval falls outside the ray's t-interval, then no need to check y or z.</p>
<p>Now, instead of rendering in 800+ seconds, given the same resources, the cow renders in 1.153 seconds... Almost 700x speedup.</p>
<div align="middle">
    <table style="width=100%">
        <tr>
            <td>
                <img src="images/part2_image1.png" width="480px" />
                <figcaption align="middle">Cow Normal Render (new)</figcaption>
            </td>
        </tr>
    </table>
</div>
<p>Two additional renders showing off the new computational speed that would have taken hours before:</p>
<div align="middle">
    <table style="width=100%">
        <tr>
            <td>
                <img src="images/part2_image2.png" width="480px" />
                <figcaption align="middle">Dragon Render</figcaption>
            </td>
            <td>
                <img src="images/part2_image3.png" width="480px" />
                <figcaption align="middle">Human Face Render</figcaption>
            </td>
        </tr>
    </table>
</div>
<p>Let's test the relationship between # of primitives and rendering time, with an additional variable of 1 or 2 camera rays per pixel.</p>
<ul>
    <li><b>TEAPOT</b> 2464 primitives, 0.852s at 1 ray/pixel and 1.845s at 2 ray/pixel.</li>
    <li><b>COW</b>: 5856 primitives, 1.149s at 1 ray/pixel and 2.559s at 2 ray/pixel.</li>
    <li><b>DRAGON</b> 100012 primitives, 0.814s at 1 ray/pixel and 1.984s at 2 ray/pixel.</li>
    <li><b>LUCY</b> 133796 primitives, 1.084s at 1 ray/pixel and 2.236s at 2 ray/pixel.</li>
</ul>
<p>Wow! One observation I made here is that the rendering speed actually seems unrelated to the number of primitives, where before, increasing the number of primitives had a detrimental effect on performance. Each render, given the same number of samples, takes approximately the same amount of time. The number of primitives did increase the time to construct a BVH, from 0.04s for the TEAPOT to about 2.0s for LUCY. Naturally, supersampling increases rendering time -- that's nothing new.</p>
<p>The performance test demonstrates the power of this acceleration technique. I could see how if there was some way to pre-process the BVH, this ray-tracer would be handier for the real-time rendering required for something graphically demanding like a video game.</p>

<h2 align="middle">Part 3: Direct Illumination</h2>
<p>The next task is to implement some realistic shading using the previously defined intersection methods. The two methods of observing the effects of light on a scene:</p>
<ul>
    <li><b>Uniform Hemisphere</b>: This method is blind to light source locations. For every point on an intersected primitive for the requested number of samples, I shoot a ray in a random direction, and if it intersects, I accumulate the light from the intersection point. If the object is not a light source, then the emitted light is 0. Hence, surfaces that are blocked or further from the light will be shaded properly.</li>
    <li><b>Importance</b>: This method actually has data about light sources. The path-tracing uses the number of requested samples slightly differently in that it takes the some consistent number of samples per light but takes the same number of overall samples. Instead of casting a ray in a completely random direction, the algorithm casts a ray at a random point on the light source. If there are no intersections between the point and the light source, then the light sampled from the source gets accumulated into the final value.</li>
</ul>
<p>Both methods use some small value epsilon to push the start of the casted ray slightly above the hitpoint, so it doesn't intersect with itself. The following image use 128 rays per pixel and 64 samples per light.</p>
<div align="middle">
    <table style="width=100%">
        <tr>
            <td>
                <img src="images/part3_image1.png" width="480px" />
                <figcaption align="middle">Uniform Hemisphere Sampling</figcaption>
            </td>
            <td>
                <img src="images/part3_image2.png" width="480px" />
                <figcaption align="middle">Importance Sampling</figcaption>
            </td>
        </tr>
    </table>
</div>
<p>Importance sampling seems to be much more effective at quickly converging to the proper lighting and eliminating noise. <b>Unfortunately, I worked very hard over the course of a week to try to eliminate the slighter darkness in the importance render, to no avail.</b> Just note that for the next two parts, the reason the importance renders are just marginally smaller is because of this. Below, we'll also examine the effect of increasing sample per area light (all sampled at 1 ray/pixel):</p>
<div align="middle">
    <table style="width=100%">
        <tr>
            <td>
                <img src="images/part3_image3.png" width="240px" />
                <figcaption align="middle">Area Samples = 1</figcaption>
            </td>
            <td>
                <img src="images/part3_image4.png" width="240px" />
                <figcaption align="middle">Area Samples = 4</figcaption>
            </td>
            <td>
                <img src="images/part3_image5.png" width="240px" />
                <figcaption align="middle">Area Samples = 16</figcaption>
            </td>
            <td>
                <img src="images/part3_image6.png" width="240px" />
                <figcaption align="middle">Area Samples = 64</figcaption>
            </td>
        </tr>
    </table>
</div>
<p>It's important to note that increasing the number of samples per light increases the accuracy of the ray-cast, but the gritty quality still remains because of aliasing from under-sampling per pixel.</p>
<div align="middle">
    <table style="width=100%">
        <tr>
            <td>
                <img src="images/part3_image7.png" width="480px" />
                <figcaption align="middle">Hemisphere Spheres</figcaption>
            </td>
            <td>
                <img src="images/part3_image8.png" width="480px" />
                <figcaption align="middle">Importance Spheres</figcaption>
            </td>
        </tr>
    </table>
</div>
<p>In conclusion, the accuracy of the lighting is apparent from both renders. However, the uniform hemisphere images generally contain more noise at the same sampling rate as the importance-sampled images. At a high level of supersampling, uniform hemisphere image would probably look indistinct from the importance-sampled image, but that's a huge computational tax. The images above should demonstrate that sampling over the light source uses far less resources and produces better quality even at low sampling.</p>

<h2 align="middle">Part 4: Global Illumination</h2>
<p>This time, we enable multiple ray-bounces. Overall, this method adds light onto the previous part to produce more realistic shading. At minimum, the path-trace will do everything from Part 3. If the specified ray is of depth > 1, then at least one additional indirect ray will be cast. The exact number of additional rays will be between 1 and m, and for the sake of runtime, there will be a small chance of ray termination (Russian roulette). The indirect ray randomly bounces off the hitpoint, recursively calling the illumination function to determine the irradiance at the next point, as determined by the sampling method (hemisphere or importance).</p>
<p>Below are hemisphere and importance samples of global illumination. Compare them to the direct illumination above.</p>
<div align="middle">
    <table style="width=100%">
        <tr>
            <td>
                <img src="images/part4_image3.png" width="480px" />
                <figcaption align="middle">Indirect Hemisphere Spheres</figcaption>
            </td>
            <td>
                <img src="images/part4_image4.png" width="480px" />
                <figcaption align="middle">Indirect Importance Spheres</figcaption>
            </td>
        </tr>
    </table>
</div>
<p>Below are the effects of observing only direct lighting vs only indirect lighting (since our algorithm combines them both for rays of depth greater than one) with max ray depth 6.</p>
<div align="middle">
    <table style="width=100%">
        <tr>
            <td>
                <img src="images/part4_image5.png" width="480px" />
                <figcaption align="middle">No Indirect</figcaption>
            </td>
            <td>
                <img src="images/part4_image6.png" width="480px" />
                <figcaption align="middle">No Direct</figcaption>
            </td>
        </tr>
    </table>
</div>
<p>Below is the rendered view for different ray-depths of the same bunny, but with combined indirect and direct lighting again.</p>
<div align="middle">
    <table style="width=100%">
        <tr>
            <td>
                <img src="images/part4_image7.png" width="200px" />
                <figcaption align="middle">Ray Depth 0</figcaption>
            </td>
            <td>
                <img src="images/part4_image8.png" width="200px" />
                <figcaption align="middle">Ray Depth 1</figcaption>
            </td>
            <td>
                <img src="images/part4_image9.png" width="200px" />
                <figcaption align="middle">Ray Depth 2</figcaption>
            </td>
            <td>
                <img src="images/part4_image10.png" width="200px" />
                <figcaption align="middle">Ray Depth 3</figcaption>
            </td>
            <td>
                <img src="images/part4_image11.png" width="200px" />
                <figcaption align="middle">Ray Depth 100</figcaption>
            </td>
        </tr>
    </table>
</div>
<p>Finally, we'll use different sampling rates for the same light to test all the work so far, using the sphere box, with max ray depth 6 and 4 lights samples.</p>
<div align="middle">
    <table style="width=100%">
        <tr>
            <td>
                <img src="images/part4_image12.png" width="240px" />
                <figcaption align="middle">S = 1</figcaption>
            </td>
            <td>
                <img src="images/part4_image13.png" width="240px" />
                <figcaption align="middle">S = 2</figcaption>
            </td>
            <td>
                <img src="images/part4_image14.png" width="240px" />
                <figcaption align="middle">S = 4</figcaption>
            </td>
            <td>
                <img src="images/part4_image15.png" width="240px" />
                <figcaption align="middle">S = 8</figcaption>
            </td>
        </tr>
        <br>
        <tr>
            <td>
                <img src="images/part4_image16.png" width="240px" />
                <figcaption align="middle">S = 16</figcaption>
            </td>
            <td>
                <img src="images/part4_image17.png" width="240px" />
                <figcaption align="middle">S = 64</figcaption>
            </td>
            <td>
                <img src="images/part4_image18.png" width="240px" />
                <figcaption align="middle">S = 264</figcaption>
            </td>
            <td>
                <img src="images/part4_image19.png" width="240px" />
                <figcaption align="middle">S = 1024</figcaption>
            </td>
        </tr>
    </table>
</div>

<h2 align="middle">Part 5: Adaptive Sampling</h2>
<p>Using a bit of statistics can cut down on computational resources if it seems like a part of a scene is pretty consistent over many samples. To do that, for every sample, I keep a running total of the pixel samples illuminance and another total of squared illuminance. With some calculations and after having taken a certain number of samples, we can calculate whether the samples are consistent enough to stop doing more computations, with 95% confidence. These checks are only made every specified number of batches of samples.</p>
<p>The final product of the first part of this project is displayed below, with sampling rate 2048, ray depth 5, adaptive sampling 8 per batch at max tolerance 0.085, and 1 sample per light.</p>
<div align="middle">
    <table style="width=100%">
        <tr>
            <td>
                <img src="images/part5_image1.png" width="480px" />
                <figcaption align="middle">Adaptive sampling in action.</figcaption>
            </td>
            <td>
                <img src="images/part5_image2.png" width="480px" />
                <figcaption align="middle">The bunny to rule all bunnies.</figcaption>
            </td>
        </tr>
    </table>
</div>
<p>The bunny is sampled at a slightly higher rate than it should be because I had to tweak the max tolerance from 0.05 to 0.085, likely due to the dimness glitch from Part 3. I ran out of time to find the exact max tolerance needed, but after many, many attempts, it is interesting to observe how increasing max tolerance causes faster render at the cost of lower sample rate (blue). This adapative approach errs on the side of too many samples (red), but shifting the max tolerance up slightly can accomodate for that.</p>
</body>
</html>
